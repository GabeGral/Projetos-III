---
output: 
  pdf_document:
    keep_tex: true
geometry: margin=1in
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(cache = TRUE, echo = FALSE, tidy = TRUE, tidy.opts = list(width.cutoff = 60), warning = T)
```

```{r, include=FALSE}
library(tidyverse)
library(caret)
library(e1071)
```

```{r}
set.seed(1444)
```

usando apenas 5 mil observacoes 

```{r read_data}
tidy_data = read_rds("tidy_data_5k")
X = tidy_data[[1]]
Y = tidy_data[[2]]
X_test = tidy_data[[3]]
Y_test = tidy_data[[4]]
```

# 3) Support Vector Machine

Por fim, usamos o Support Vector Machine. Para este algoritmo, a escolha do Kernel é algo especialmente importante. Além dele, também preciamos escolher o custo, como é chamado o parâmetro que dá o peso da penalização por uma separação imperfeita das categorias. 

Fizemos um down sampling pra balancear as categorias por questões computacionais...

```{r}
sample.d = downSample(x = X, y = Y, list = TRUE)
X = sample.d$x
Y = sample.d$y
```


## Linear Kernel

Começando pela versão mais simples com o Kernel linear, usamos um 10-fold cross-validation para escolher o parâmetro Custo.

\textcolor{red}{tive q dropar umas variaveis que nao eram padronizaveis pq não tinham variancia}

```{r}
a = map_df(data.frame(X), sd) %>% as.vector()
a <- a>=0.02
X = X[ , a] #removendo do X as variaveis que dariam problema na padronizacao (desvio padrao menor que 2%)
```


```{r, eval=FALSE}
# cost = 1
svm_linear = svm(x= X.norm, y= as.factor(Y), kernel = "linear", cost = 1 )
```

```{r, eval = FALSE}
#in sample
y_pred_in.sample = predict(svm_linear, newx = X.norm)
conf_matrix = confusionMatrix( data = y_pred_in.sample, 
                  reference = as.factor(Y))
conf_matrix$table
conf_matrix$overall[[1]]
```

```{r cost.cv}
# tune model to find optimal cost
 # 10-folds cross-validation
tuning <- tune(svm, train.x= X, train.y= as.factor(Y), 
               kernel = "linear",
               ranges = list(cost = c( 0.1, 1, 5,
                                      10, 15, 20, 30, 40, 50)),
                tune.control(sampling = "cross", cross = 10) )
#tuning$best.model
```

```{r}
cost_linear = tuning$performances[ which.min(tuning$performances$error), 1]
error_linear = tuning$performances[ which.min(tuning$performances$error), 2]
```

```{r}
ggplot( tuning$performances[ , 1:2], aes(x= cost, y = error))+
  geom_point()+
  geom_point(aes(x= tuning$performances[which.min(tuning$performances$error) , 1], y=min(tuning$performances$error)), color = "red")
```

Nesse caso, o custo ótimo foi `r cost_linear`.

## Tentando outros Kernels

Tentamos outros Kernels não-lineares, escolhendo seus parâmetros mais uma vez por 10-fold cross-validation:

* Polynomial: \((\gamma u'v + coef_0)^{degree}\)
  + escolher: \(degree\), \(\gamma\), \(coef_0\)
* Radial basis: \(e^{(-\gamma |u-v|^2)}\)
  + escolher: \(\gamma\)
* Sigmoid (hyperbolic tangent): \(tanh(\gamma u'v + coef_0)\) 
  + escolher: \(\gamma\), \(coef_0\)

O Kernel polinomial estava levando muito tempo para encontrar uma solução, quando a encontrava, e por isso foi abandonado.

Também por uma questão computacional, os custos testados no cross-validation foram apenas o default (custo = 1) e o custo ótimo encontrado para o Kernel linear (custo = `r cost_linear`).

```{r}
#setting ranges for the parameters
parameters_pol = list(cost = c(1, cost_linear), 
                      gamma = 10^(c(-1:1)), 
                      degree = 1:3, 
                      coef0 = seq(from = 0, to = 100, by = 50) )
parameters_rad = list(cost = c(1, cost_linear), 
                      gamma = c(0.1, 0.5, 1, 2, 10) )
parameters_sig = list(cost = c(1, cost_linear), 
                      gamma = 10^(-2:2), 
                      coef0 = seq(from = 0, to = 100, by = 25) )
```

```{r}
best = list()
```

```{r polynomial.cv, eval = FALSE}
# polynomial
# 10-fold CV
tuning <- tune(svm, train.x= X, train.y= as.factor(Y), 
               kernel = "polynomial",
               ranges = parameters_pol,
                tune.control(sampling = "cross", cross = 10) )
#tuning$best.parameters
#tuning$performances
best[["polynomial"]] = tuning$performances[which.min(tuning$performances$error), ] 
```

```{r radial.cv}
# radial
# 10-fold CV
tuning <- tune(svm, train.x= X, train.y= as.factor(Y), 
               kernel = "radial",
               ranges = parameters_rad,
                tune.control(sampling = "cross", cross = 10) )
#tuning$best.parameters
#tuning$performances
best[["radial"]] = tuning$performances[which.min(tuning$performances$error), ]
```

```{r sigmoid.cv}
# sigmoid
# 10-fold CV
tuning <- tune(svm, train.x= X, train.y= as.factor(Y), 
               kernel = "sigmoid",
               ranges = parameters_sig,
                tune.control(sampling = "cross", cross = 10) )
#tuning$best.parameters
#tuning$performances
best[["sigmoid"]] = tuning$performances[which.min(tuning$performances$error), ]
```

Comparando os diferentes Kernels, considerndo os melhores parâmetros para cada um, temos:

```{r}
print(best)
```

Lembrando que o erro de validação do Kernel linear ótimo (custo = `r cost_linear`) foi `r error_linear`.

## Modelo final

Finalmente, usarei o modelo com kernel do tipo linear, com o custo igual a 15, como este foi o que obteve o menor erro de validação dentre todas as especificações testadas.

```{r final_svm}
# svm = svm(x= X, y= as.factor(Y), kernel = "polynomial", 
#           cost = 10, gamma = 10, degree = 1, coef0 = 0)
svm = svm(x= X, y= as.factor(Y), kernel = "linear", cost = cost_linear )
```

Aplicando este modelo aos dados da base de teste, temos:

**Confusion Matrix**

```{r predict}
# X_test.norm = rbind(X_test, X) %>%
#   scale( center = T, scale = T)
#   
# X_test.norm = X_test.norm[ 1:nrow(X_test), a] 

X_test = X_test[ , a] #removendo as mesmas variaveis que foram removidas no training set

y_pred = predict(svm, X_test)
conf_matrix = confusionMatrix( data = as.factor(y_pred), 
                  reference = as.factor(Y_test))
conf_matrix$table
accuracy = conf_matrix$overall[1]
```

Logo, a acurácia fora da amostra foi `r accuracy`.

Fazendo para o mesmo test set dos outros, temos:

```{r read_data-2}
tidy_data = read_rds("tidy_data")
X = tidy_data[[1]]
Y = tidy_data[[2]]
X_test = tidy_data[[3]]
Y_test = tidy_data[[4]]
```


```{r predict-2}
# X_test.norm = rbind(X_test, X) %>%
#   scale( center = T, scale = T)
#   
# X_test.norm = X_test.norm[ 1:nrow(X_test), a] 

X_test = X_test[ , a] #removendo as mesmas variaveis que foram removidas no training set

y_pred = predict(svm, X_test)
conf_matrix = confusionMatrix( data = as.factor(y_pred), 
                  reference = as.factor(Y_test))
conf_matrix$table
accuracy = conf_matrix$overall[1]
```

A acurácia na mesma base de test foi `r accuracy`.
























